#!/usr/bin/env python3
"""
æ¶ˆæ¯ç”Ÿäº§è€…å·¥å…·

å‘ä¸»é˜Ÿåˆ—å‘é€æµ‹è¯•æ¶ˆæ¯ï¼Œç”¨äºæµ‹è¯•æ•´ä¸ªåˆ†å‘ç³»ç»Ÿã€‚
"""
import argparse
import json
import uuid
import time
from datetime import datetime
from typing import Dict, List
import boto3
from botocore.exceptions import ClientError


class MessageProducer:
    """æ¶ˆæ¯ç”Ÿäº§è€…ç±»"""

    def __init__(self, queue_url: str, profile: str = "default"):
        """
        åˆå§‹åŒ–ç”Ÿäº§è€…

        Args:
            queue_url: ä¸»é˜Ÿåˆ— URL
            profile: AWS profile åç§°
        """
        session = boto3.Session(profile_name=profile)
        self.sqs = session.client("sqs")
        self.queue_url = queue_url
        self.stats = {"sent": 0, "failed": 0}

    def generate_message(
        self,
        model_name: str = "gpt-l-7b",
        priority: str = "normal",
        custom_request_id: str = None
    ) -> Dict:
        """
        ç”Ÿæˆæµ‹è¯•æ¶ˆæ¯

        Args:
            model_name: æ¨¡å‹åç§°
            priority: ä¼˜å…ˆçº§
            custom_request_id: è‡ªå®šä¹‰ request_idï¼ˆç”¨äºæµ‹è¯•å¹‚ç­‰æ€§ï¼‰

        Returns:
            Dict: æ¶ˆæ¯å­—å…¸
        """
        request_id = custom_request_id or f"req-{uuid.uuid4()}"

        message = {
            "request_id": request_id,
            "model_name": model_name,
            "input_data_url": f"s3://test-bucket/inputs/{request_id}.json",
            "callback_sns_topic": "arn:aws:sns:us-east-1:123456789012:inference-results",
            "priority": priority,
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "metadata": {
                "user_id": f"user-{uuid.uuid4().hex[:8]}",
                "session_id": f"session-{uuid.uuid4().hex[:8]}",
                "test_source": "producer.py"
            }
        }

        return message

    def send_message(self, message: Dict) -> bool:
        """
        å‘é€å•æ¡æ¶ˆæ¯

        Args:
            message: æ¶ˆæ¯å­—å…¸

        Returns:
            bool: æ˜¯å¦å‘é€æˆåŠŸ
        """
        try:
            response = self.sqs.send_message(
                QueueUrl=self.queue_url,
                MessageBody=json.dumps(message)
            )

            print(f"âœ“ å‘é€æˆåŠŸ: {message['request_id']} (MessageId: {response['MessageId']})")
            self.stats["sent"] += 1
            return True

        except ClientError as e:
            print(f"âœ— å‘é€å¤±è´¥: {message['request_id']} - {e}")
            self.stats["failed"] += 1
            return False

    def send_batch(self, messages: List[Dict]) -> Dict:
        """
        æ‰¹é‡å‘é€æ¶ˆæ¯ï¼ˆæœ€å¤š 10 æ¡ï¼‰

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨

        Returns:
            Dict: å‘é€ç»“æœç»Ÿè®¡
        """
        if len(messages) > 10:
            raise ValueError("æ‰¹é‡å‘é€æœ€å¤šæ”¯æŒ 10 æ¡æ¶ˆæ¯")

        entries = [
            {
                "Id": msg["request_id"],
                "MessageBody": json.dumps(msg)
            }
            for msg in messages
        ]

        try:
            response = self.sqs.send_message_batch(
                QueueUrl=self.queue_url,
                Entries=entries
            )

            # å¤„ç†æˆåŠŸçš„æ¶ˆæ¯
            for success in response.get("Successful", []):
                print(f"âœ“ æ‰¹é‡å‘é€æˆåŠŸ: {success['Id']}")
                self.stats["sent"] += 1

            # å¤„ç†å¤±è´¥çš„æ¶ˆæ¯
            for failed in response.get("Failed", []):
                print(f"âœ— æ‰¹é‡å‘é€å¤±è´¥: {failed['Id']} - {failed.get('Message')}")
                self.stats["failed"] += 1

        except ClientError as e:
            print(f"âœ— æ‰¹é‡å‘é€å¼‚å¸¸: {e}")
            self.stats["failed"] += len(messages)

        return self.stats

    def run_continuous(
        self,
        count: int,
        interval: float = 0.5,
        batch_size: int = 1,
        model_name: str = "gpt-l-7b"
    ) -> None:
        """
        æŒç»­å‘é€æ¶ˆæ¯

        Args:
            count: å‘é€æ¶ˆæ¯æ€»æ•°
            interval: å‘é€é—´éš”ï¼ˆç§’ï¼‰
            batch_size: æ‰¹é‡å¤§å°ï¼ˆ1-10ï¼‰
            model_name: æ¨¡å‹åç§°
        """
        print(f"\nå¼€å§‹å‘é€æ¶ˆæ¯...")
        print(f"æ€»æ•°: {count}, é—´éš”: {interval}s, æ‰¹é‡å¤§å°: {batch_size}\n")

        sent = 0
        while sent < count:
            remaining = count - sent
            current_batch_size = min(batch_size, remaining, 10)

            if current_batch_size == 1:
                # å•æ¡å‘é€
                message = self.generate_message(model_name=model_name)
                self.send_message(message)
                sent += 1
            else:
                # æ‰¹é‡å‘é€
                messages = [
                    self.generate_message(model_name=model_name)
                    for _ in range(current_batch_size)
                ]
                self.send_batch(messages)
                sent += current_batch_size

            if sent < count:
                time.sleep(interval)

        print(f"\nâœ… å‘é€å®Œæˆï¼")
        print(f"æˆåŠŸ: {self.stats['sent']}, å¤±è´¥: {self.stats['failed']}")


def main():
    parser = argparse.ArgumentParser(
        description="æ¶ˆæ¯ç”Ÿäº§è€… - å‘ä¸»é˜Ÿåˆ—å‘é€æµ‹è¯•æ¶ˆæ¯"
    )

    parser.add_argument(
        "--queue-url",
        required=True,
        help="ä¸»é˜Ÿåˆ— URL"
    )

    parser.add_argument(
        "--count",
        type=int,
        default=10,
        help="å‘é€æ¶ˆæ¯æ•°é‡ï¼ˆé»˜è®¤: 10ï¼‰"
    )

    parser.add_argument(
        "--interval",
        type=float,
        default=0.5,
        help="å‘é€é—´éš”ç§’æ•°ï¼ˆé»˜è®¤: 0.5ï¼‰"
    )

    parser.add_argument(
        "--batch-size",
        type=int,
        default=1,
        choices=range(1, 11),
        help="æ‰¹é‡å¤§å° 1-10ï¼ˆé»˜è®¤: 1ï¼‰"
    )

    parser.add_argument(
        "--model",
        default="gpt-l-7b",
        help="æ¨¡å‹åç§°ï¼ˆé»˜è®¤: gpt-l-7bï¼‰"
    )

    parser.add_argument(
        "--profile",
        default="default",
        help="AWS profile åç§°ï¼ˆé»˜è®¤: defaultï¼‰"
    )

    parser.add_argument(
        "--duplicate",
        action="store_true",
        help="å‘é€é‡å¤æ¶ˆæ¯ï¼ˆæµ‹è¯•å¹‚ç­‰æ€§ï¼‰"
    )

    parser.add_argument(
        "--request-id",
        help="è‡ªå®šä¹‰ request_idï¼ˆç”¨äºæµ‹è¯•å¹‚ç­‰æ€§ï¼‰"
    )

    args = parser.parse_args()

    producer = MessageProducer(args.queue_url, args.profile)

    if args.duplicate and args.request_id:
        # æµ‹è¯•å¹‚ç­‰æ€§ï¼šå‘é€å¤šæ¡ç›¸åŒ request_id çš„æ¶ˆæ¯
        print(f"\nğŸ”„ å¹‚ç­‰æ€§æµ‹è¯•æ¨¡å¼ï¼šå‘é€ {args.count} æ¡ç›¸åŒçš„æ¶ˆæ¯")
        print(f"Request ID: {args.request_id}\n")

        for i in range(args.count):
            message = producer.generate_message(
                model_name=args.model,
                custom_request_id=args.request_id
            )
            producer.send_message(message)
            if i < args.count - 1:
                time.sleep(args.interval)
    else:
        # æ­£å¸¸æ¨¡å¼
        producer.run_continuous(
            count=args.count,
            interval=args.interval,
            batch_size=args.batch_size,
            model_name=args.model
        )


if __name__ == "__main__":
    main()
